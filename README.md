# 🛒 E-commerce Data Pipeline with Kafka, Docker & Snowflake

This project demonstrates an **e-commerce data pipeline** that simulates order data using **Kafka**, processes it with **Docker**, and loads it into **Snowflake** for further analytics.

## 📦 Project Overview

The project simulates an e-commerce system where orders are generated by a **Kafka Producer** and consumed by a **Kafka Consumer**. The consumer then loads the order data into **Snowflake** for analytics and storage.

- **Producer** generates fake order data and sends it to a Kafka topic.
- **Consumer** retrieves the data from Kafka and loads it into Snowflake.
- **Docker** is used to spin up Kafka and Zookeeper for easy local setup.

---

## ⚙️ Tech Stack

- 🐍 **Python**: Programming language for the producer and consumer.
- 🐳 **Docker**: Containerization for Kafka and Zookeeper.
- 📨 **Apache Kafka**: Message broker for real-time data streaming.
- ❄️ **Snowflake**: Data warehousing for storing the orders.
- 🌐 **Environment Variables**: Used for Snowflake credentials.
- 📜 **SQL**: For interacting with Snowflake.

---

## 🚀 How to Run

Follow these steps to run the project locally:

### 1. Clone the repo

```bash
git clone https://github.com/Sk928551/ecommerce-data-pipeline

2. Set up your Snowflake .env file
Create a .env file in the root directory using the format in .env.example. It should look like this:


SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_ACCOUNT=your_account_identifier
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
3. Start Kafka & Zookeeper using Docker
To start Kafka and Zookeeper, run the following command:


docker-compose up -d
This will set up the required containers in the background.

4. Start the Kafka Consumer (Snowflake Loader)
Run the Kafka consumer to start receiving messages from the Kafka topic and load them into Snowflake:


cd consumer
python3 ecommerce_consumer.py
5. Start the Kafka Producer (Generate Fake Orders)
Now, generate fake order data and send it to Kafka:


cd producer
python3 ecommerce_producer.py
📊 Sample Data
A sample order generated by the system:

json
Copy
Edit
{
  "order_id": 101,
  "customer_name": "Alice",
  "product": "Laptop",
  "amount": 1000,
  "order_date": "2024-05-01"
}
This data is continuously pushed to the Kafka topic and then loaded into Snowflake by the consumer.

🧠 What I Learned
Kafka Streaming: Understanding how to stream real-time data between systems using Kafka.

Docker & Containerization: Spinning up Kafka and Zookeeper easily with Docker for local development.

Snowflake: How to connect and load data into Snowflake for analytics using Python.

Data Pipeline Architecture: Building a simple end-to-end pipeline with real-time data flow and storage.

✅ Features
Simulates 100+ orders with random data.

Kafka Producer to generate real-time order data.

Kafka Consumer to consume data and insert into Snowflake.

Dockerized Kafka and Zookeeper setup for easy local development.

🙋‍♂️ Author
Aman Kanthiwar
📧 amankanthiwar78@gmail.com
